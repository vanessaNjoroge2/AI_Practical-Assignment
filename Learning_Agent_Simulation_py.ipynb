{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQB7H0xUR3Yw0/hjG49l2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanessaNjoroge2/AI_Practical-Assignment/blob/main/Learning_Agent_Simulation_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5wKRVemuzYb",
        "outputId": "1c0f39b0-55e9-44f2-8524-bce209d0cb48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = -2.30, Steps taken = 34\n",
            "Episode 2: Total Reward = 0.60, Steps taken = 5\n",
            "Episode 3: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 4: Total Reward = 0.30, Steps taken = 8\n",
            "Episode 5: Total Reward = 0.60, Steps taken = 5\n",
            "Episode 6: Total Reward = 0.60, Steps taken = 5\n",
            "Episode 7: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 8: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 9: Total Reward = 0.50, Steps taken = 6\n",
            "Episode 10: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 11: Total Reward = 0.50, Steps taken = 6\n",
            "Episode 12: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 13: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 14: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 15: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 16: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 17: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 18: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 19: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 20: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 21: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 22: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 23: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 24: Total Reward = 0.50, Steps taken = 6\n",
            "Episode 25: Total Reward = 0.30, Steps taken = 8\n",
            "Episode 26: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 27: Total Reward = 0.60, Steps taken = 5\n",
            "Episode 28: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 29: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 30: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 31: Total Reward = 0.60, Steps taken = 5\n",
            "Episode 32: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 33: Total Reward = 0.30, Steps taken = 8\n",
            "Episode 34: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 35: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 36: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 37: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 38: Total Reward = 0.30, Steps taken = 8\n",
            "Episode 39: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 40: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 41: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 42: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 43: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 44: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 45: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 46: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 47: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 48: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 49: Total Reward = 0.70, Steps taken = 4\n",
            "Episode 50: Total Reward = 0.30, Steps taken = 8\n",
            "\n",
            "Trained Q-Table:\n",
            "[[[ 0.08477132 -0.13875     0.07219687  0.458     ]\n",
            "  [-0.0733125   0.62        0.18981191 -0.075     ]\n",
            "  [-0.05       -0.075       0.41969222  0.        ]]\n",
            "\n",
            " [[-0.0975     -0.11       -0.0975      0.2725    ]\n",
            "  [-0.05        0.525      -0.10875     0.8       ]\n",
            "  [ 0.16349548  1.          0.31        0.        ]]\n",
            "\n",
            " [[-0.0975     -0.0975     -0.0975     -0.075     ]\n",
            "  [-0.05       -0.05       -0.0725      0.9375    ]\n",
            "  [ 0.          0.          0.          0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the environment\n",
        "# 3x3 grid (0: empty, 1: goal)\n",
        "grid_size = 3\n",
        "goal_state = (2, 2)  # bottom-right corner is the goal\n",
        "\n",
        "# Initialize Q-table (states x actions)\n",
        "# Actions: 0=up, 1=down, 2=left, 3=right\n",
        "Q = np.zeros((grid_size, grid_size, 4))\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.5        # Learning rate\n",
        "gamma = 0.9        # Discount factor\n",
        "epsilon = 0.2      # Exploration probability\n",
        "episodes = 50      # Number of trials\n",
        "\n",
        "# Function to choose action (epsilon-greedy)\n",
        "def choose_action(state):\n",
        "    if random.uniform(0,1) < epsilon:\n",
        "        return random.randint(0,3)  # Explore: random action\n",
        "    else:\n",
        "        x, y = state\n",
        "        return np.argmax(Q[x, y])   # Exploit: best action\n",
        "\n",
        "# Function to take action and return next state and reward\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    if action == 0 and x > 0:          # up\n",
        "        x -= 1\n",
        "    elif action == 1 and x < grid_size-1:  # down\n",
        "        x += 1\n",
        "    elif action == 2 and y > 0:        # left\n",
        "        y -= 1\n",
        "    elif action == 3 and y < grid_size-1:  # right\n",
        "        y += 1\n",
        "\n",
        "    next_state = (x, y)\n",
        "    reward = 1 if next_state == goal_state else -0.1  # small penalty for each move\n",
        "    done = next_state == goal_state\n",
        "    return next_state, reward, done\n",
        "\n",
        "# Training loop\n",
        "for episode in range(episodes):\n",
        "    state = (0, 0)  # Start at top-left corner\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # Q-learning update\n",
        "        x, y = state\n",
        "        nx, ny = next_state\n",
        "        Q[x, y, action] = Q[x, y, action] + alpha * (reward + gamma * np.max(Q[nx, ny]) - Q[x, y, action])\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Episode {episode+1}: Total Reward = {total_reward:.2f}, Steps taken = {steps}\")\n",
        "\n",
        "print(\"\\nTrained Q-Table:\")\n",
        "print(Q)\n"
      ]
    }
  ]
}